{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Frozen lake\n",
    "The agent controls the movement of a character in a grid world. Some tiles of the grid are walkable, and others lead to the agent falling into the water. Additionally, the movement direction of the agent is uncertain and only partially depends on the chosen direction. The agent is rewarded for finding a walkable path to a goal tile.<br /><br/>The surface is described using a grid like the following<br />SFFF<br />        FHFH<br />        FFFH<br />        HFFG<br/>S : starting point, safe<br />F : frozen surface, safe<br />H : hole, fall to your doom<br />G : goal, where the frisbee is located<br />The episode ends when you reach the goal or fall in a hole.<br />You receive a reward of 1 if you reach the goal, and zero otherwise.<br/><br/>Possible steps:<br/>LEFT = 0<br />DOWN = 1<br />RIGHT = 2<br />UP = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "env = gym.make('FrozenLake-v0')\n",
    "env.reset()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action Space Count: 4\n",
      "Observation Space Count: 16\n"
     ]
    }
   ],
   "source": [
    "# Action Space and Observation Space\n",
    "# Action Space Bound - [0,3]\n",
    "# Observation Space Bound - [0,15]\n",
    "action_space = env.action_space.n\n",
    "obs_space = env.observation_space.n\n",
    "print(\"Action Space Count: \"+str(action_space))\n",
    "print(\"Observation Space Count: \"+str(obs_space))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manual Approach\n",
    "Demonstration of every action that can be taken by the agent in the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial State:\n",
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "print(\"Initial State:\")\n",
    "env.reset()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "State: 0\n",
      "Reward: 0.0\n",
      "Done: False\n",
      "Info: {'prob': 0.3333333333333333}\n"
     ]
    }
   ],
   "source": [
    "state,reward,done,info = env.step(0)\n",
    "env.render()\n",
    "print(\"\\nState: \"+str(state)+\"\\nReward: \"+str(reward)+\"\\nDone: \"+str(done)+\"\\nInfo: \"+str(info))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Down)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "State: 4\n",
      "Reward: 0.0\n",
      "Done: False\n",
      "Info: {'prob': 0.3333333333333333}\n"
     ]
    }
   ],
   "source": [
    "state,reward,done,info = env.step(1)\n",
    "env.render()\n",
    "print(\"\\nState: \"+str(state)+\"\\nReward: \"+str(reward)+\"\\nDone: \"+str(done)+\"\\nInfo: \"+str(info))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Right)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "State: 0\n",
      "Reward: 0.0\n",
      "Done: False\n",
      "Info: {'prob': 0.3333333333333333}\n"
     ]
    }
   ],
   "source": [
    "state,reward,done,info = env.step(2)\n",
    "env.render()\n",
    "print(\"\\nState: \"+str(state)+\"\\nReward: \"+str(reward)+\"\\nDone: \"+str(done)+\"\\nInfo: \"+str(info))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Up)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "State: 0\n",
      "Reward: 0.0\n",
      "Done: False\n",
      "Info: {'prob': 0.3333333333333333}\n"
     ]
    }
   ],
   "source": [
    "state,reward,done,info = env.step(3)\n",
    "env.render()\n",
    "print(\"\\nState: \"+str(state)+\"\\nReward: \"+str(reward)+\"\\nDone: \"+str(done)+\"\\nInfo: \"+str(info))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see that agent not always moves in  the direction of action specified, the direction in which agent goes is not deterministic but probabilistic (because of the fact that ice is slippery). It is difficult to manually solve this problem as it will take many steps. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Agent\n",
    "Taking random action from the action space till the episode is completed. Episode is considered done when the agent reaches the goal or fall in hole."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial State:\n",
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "Final State:\n",
      "  (Down)\n",
      "SFFF\n",
      "F\u001b[41mH\u001b[0mFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "Total Steps: 13\n",
      "State: 5\n",
      "Reward: 0.0\n",
      "Done: True\n",
      "Info: {'prob': 0.3333333333333333}\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "print(\"Initial State:\")\n",
    "env.render()\n",
    "reward = 0\n",
    "count = 0 \n",
    "done = False\n",
    "while not done:\n",
    "    action = env.action_space.sample()\n",
    "    state,reward,done,info = env.step(action)\n",
    "    count +=1\n",
    "print(\"\\nFinal State:\")\n",
    "env.render()\n",
    "print(\"\\nTotal Steps: \"+str(count)+\"\\nState: \"+str(state)+\"\\nReward: \"+str(reward)+\"\\nDone: \"+str(done)+\"\\nInfo: \"+str(info))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the agent took 13 steps and fell in hole therefore reward is zero. This problem cannot be solved by a Random Agent because of the probabilistic nature of the environment. This can be solved by the Value Iteration Agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.10 ('data-science')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "1ba2b7840eef77421e06f337bfc5ba96d44c69d7e81c0f8040b4451e2c00e338"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
